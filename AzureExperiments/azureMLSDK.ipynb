{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AzureML using SDK\n",
    "\n",
    "This is a document of implement an AzureML workflow using SDK.\n",
    "\n",
    "**Steps**\n",
    "\n",
    "1. Create Workspace \n",
    "2. Create Datastore\n",
    "3. Create Dataset\n",
    "4. Create Compute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an AzureML Workspace\n",
    "\n",
    "* **Workspace**\n",
    "* **Config** file: used for other creation of datastore, dataset and so on.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace.create(\n",
    "    name='myworkspace',\n",
    "    subscription_id='<azure-subscription-id>',\n",
    "    resource_group='myresourcegroup',\n",
    "    create_resource_group=True,\n",
    "    location='eastus2'\n",
    ")\n",
    "\n",
    "# save the config file to local directory used for other creation of datastore and so on\n",
    "ws.write_config(path='./config')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an Azure Datastore\n",
    "\n",
    "* Azure Blob Container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace, Datastore\n",
    "\n",
    "# get workspace from config file\n",
    "ws = Workspace.from_config('./config')\n",
    "\n",
    "# Create a datastore\n",
    "az_store = Datastore.register_azure_blob_container(\n",
    "    workspace=ws,\n",
    "    datastore_name='azure_sdk_blob01',\n",
    "    container_name='azuremlstb01blob',\n",
    "    account_name='azuremllsb01',\n",
    "    account_key='abc-bbc-aba'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dataset\n",
    "\n",
    "1. Access to Workspace\n",
    "2. Access to Datastore\n",
    "3. Create dataset under datastore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace, Datastore, Dataset\n",
    "\n",
    "# Access Workspace\n",
    "ws = Workspace.from_config('./config')\n",
    "\n",
    "# Access datastore\n",
    "az_store = Datastore.get(ws, \"azure_sdk_blob01\")\n",
    "\n",
    "# Create the path of the csv file\n",
    "csv_path = [(az_store, \"Loan Data/Loan Approval Prediction.csv\")]\n",
    "\n",
    "# Create the dataset\n",
    "loan_dataset = Dataset.Tabular.from_delimited_files(path=csv_path)\n",
    "\n",
    "# Register the dataset to workspace\n",
    "loan_dataset = loan_dataset.register(workspace=ws,\n",
    "                                     name=\"loan Applications Using SDK\",\n",
    "                                     create_new_version=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Access Workspace, Datastore, Datasets using SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------\n",
    "# Access wokrspace by name\n",
    "# ---------------------------------------\n",
    "\n",
    "ws = Workspace.from_config('./config')\n",
    "\n",
    "# List all workspaces within a subcription\n",
    "ws_list = Workspace.list(subscription_id=\"4235-234\")\n",
    "\n",
    "\n",
    "# ---------------------------------------\n",
    "# Access the default datastore from workspace\n",
    "# ---------------------------------------\n",
    "\n",
    "az_default_store = ws.get_default_datastore()\n",
    "\n",
    "# List all datastore\n",
    "store_list = list(ws.datastores)\n",
    "\n",
    "\n",
    "# ---------------------------------------\n",
    "# Access datasets\n",
    "# ---------------------------------------\n",
    "\n",
    "# get dataset by name from a workspace\n",
    "az_dataset = Dataset.get_by_name(ws, \"Loan Applications Using SDK\")\n",
    "\n",
    "# list datasets from workspace\n",
    "dataset_list = list(ws.datastores.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas DataFrame in and out of AzureML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Workspace.from_config('./config')\n",
    "az_store = Datastore.get(ws, \"azure_sdk_blob01\")\n",
    "az_dataset = Dataset.get_by_name(ws, \"Loan Applications Using SDK\")\n",
    "az_default_store = ws.get_default_datastore()\n",
    "\n",
    "# ---------------------------------------\n",
    "# dataset from and to DataFrame\n",
    "# ---------------------------------------\n",
    "\n",
    "# Load AzureML dataset into the pandas DataFrame\n",
    "df = az_dataset.to_pandas_dataframe()\n",
    "\n",
    "# Upload DataFrame to the AzureML dataset\n",
    "az_ds_from_df = Dataset.Tabular.register_pandas_dataframe(\n",
    "    dataframe=df,\n",
    "    target=az_store,\n",
    "    name='upload_df_dataset')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload local file to storage account via datastore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Workspace.from_config('./config')\n",
    "az_store = Datastore.get(ws, \"azure_sdk_blob01\")\n",
    "az_dataset = Dataset.get_by_name(ws, \"Loan Applications Using SDK\")\n",
    "az_default_store = ws.get_default_datastore()\n",
    "\n",
    "# Upload local file\n",
    "files_list = ['./data/test1.csv', './data/test2.csv']\n",
    "\n",
    "az_store.upload_files(file=files_list,\n",
    "                      target_path='loan Data/',\n",
    "                      relative_root='./data/',\n",
    "                      overwrite=True)\n",
    "\n",
    "# Upload local folder\n",
    "az_store.upload(src_dir='./data',\n",
    "                target_path='loan Data/data',\n",
    "                 overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Experiment\n",
    "\n",
    "Create the experiment on localmachin\n",
    "* Get the datasets\n",
    "* Log the results\n",
    "\n",
    "Finally access to Workspace "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create/Access  an Experiment object\n",
    "from azureml.core import Experiment\n",
    "\n",
    "experiment = Experiment(workspace=ws,\n",
    "                        name='Loan-SDK-Exp01'\n",
    "                        )\n",
    "\n",
    "# ---------------------------------------\n",
    "# Run an experiment\n",
    "# ---------------------------------------\n",
    "\n",
    "# Start an experiment run\n",
    "new_run = experiment.start_logging()\n",
    "\n",
    "# stuff\n",
    "df = az_dataset.to_pandas_dataframe()\n",
    "\n",
    "# Log the metrics to the workspace\n",
    "new_run.log('Total obserrvations:', len(df))\n",
    "\n",
    "# Complete an experiment run\n",
    "new_run.complete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run an experiment from config script\n",
    "\n",
    "* Create a Workspacke\n",
    "* Create an Experiment using config script\n",
    "  * Objective an experiment\n",
    "  * Submit an experiment by a config script\n",
    "  * Submit an run in the existing running experiment, should run under the submit experiment, directly submit a Run() experiment   \n",
    "    ```python Run.get_context() ``` will treat all the script submit to the existing runing experiment, no need to start a new run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace, Experiment, ScriptRunConfig\n",
    "\n",
    "# ---------------------------------------\n",
    "# Access the workspace using config.json\n",
    "# ---------------------------------------\n",
    "\n",
    "ws = Workspace.from_config('./config') # in below\n",
    "\n",
    "# ---------------------------------------\n",
    "# Create and submit an experiment\n",
    "# ---------------------------------------\n",
    "\n",
    "new_experiment = Experiment(workspace=ws, name=\"new_expriment\")\n",
    "script_config = ScriptRunConfig(source_directory='.', script='experiment run script.py')\n",
    "\n",
    "new_run = new_experiment.submit(config=script_config)\n",
    "\n",
    "new_run.wait_for_completion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- experiment run script.py Submit a script to the existing run\n",
    "\n",
    "# Create/Access  an Experiment object\n",
    "from azureml.core import Experiment, Run\n",
    "\n",
    "# ---------------------------------------\n",
    "# Run an experiment\n",
    "# ---------------------------------------\n",
    "\n",
    "# Embedded a run into an existing running experiment\n",
    "new_run = Run.get_context()\n",
    "\n",
    "# stuff\n",
    "df = az_dataset.to_pandas_dataframe()\n",
    "\n",
    "# Log the metrics to the workspace\n",
    "new_run.log('Total obserrvations:', len(df))\n",
    "\n",
    "# Complete an experiment run\n",
    "new_run.complete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Script as Training Model to AzureML\n",
    "\n",
    "On previous # stuff part, we can chagne to local training model script\n",
    "\n",
    "* Access the Workspace\n",
    "* Create and register custom environment, and dependencies \n",
    "* Get then context of the experiment run\n",
    "* Local training and prediction procedures(# stuff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required classes from AzureML\n",
    "from azureml.core import Workspace, Experiment, Run, ScriptRunConfig, Environment\n",
    "from azureml.core.environment import CondaDependencies\n",
    "\n",
    "# Access the Workspace\n",
    "ws = Workspace.from_config('./config')\n",
    "\n",
    "# Create and register a custom environment\n",
    "myenv = Environment(name='mytrainingenv')\n",
    "\n",
    "# Create the dependencies\n",
    "my_dep = CondaDependencies.create(conda_packages=['scikit-learn'])\n",
    "myenv.python.conda_dependencies = my_dep\n",
    "\n",
    "# Register the environment\n",
    "myenv.register(ws)\n",
    "\n",
    "# Submit Run Experiment from script\n",
    "new_exp = Experiment(Workspace=ws, name='Prediction_to_run')\n",
    "script_config = ScriptRunConfig(source_directory='.', script='training.py')\n",
    "\n",
    "new_run = new_exp.submit(config=script_config)\n",
    "\n",
    "new_run.wait_for_complete()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training.py\n",
    "\n",
    "# get context of experiment run\n",
    "new_run = Run.get_context()\n",
    "\n",
    "# Training stuffs here\n",
    "\n",
    "# Logging run\n",
    "new_run.log('Total observation')\n",
    "new_run.log('metrics:')\n",
    "\n",
    "new_run.complete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Provision a Compute Cluster\n",
    "\n",
    "* Access the Workspace\n",
    "* Create a Compute Cluster\n",
    "* Attach the Compute Cluster to an Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the Workspace\n",
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace.from_config('./config')\n",
    "\n",
    "# Provision a Compute Cluster\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "\n",
    "# Configuration of the compute cluster\n",
    "compute_config = AmlCompute.provisioning_configuration(\n",
    "    vm_size='STANDARD_D11_V2',\n",
    "    max_nodes=2\n",
    ")\n",
    "\n",
    "compute_target = ComputeTarget.create(\n",
    "    workspace=ws,\n",
    "    name='aml-cluster' \n",
    ")\n",
    "\n",
    "# Create the cluster\n",
    "computer_cluster = ComputeTarget.create(ws, name='my-cluster-001', compute_config=compute_config)\n",
    "\n",
    "computer_cluster.wait_for_completion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automate Model Training\n",
    "\n",
    "Build two pipelines: \n",
    "* Data procsssing ```DataPrep.py```\n",
    "  * Read the data\n",
    "  * Select/Drop columns\n",
    "  * Replace Missing values\n",
    "  * Normalize the data\n",
    "  * Upload the data and log metrics\n",
    "  * Save and Pass the data to the next step\n",
    "  \n",
    "* Build and train model ```Training.py```\n",
    "  * Read the data saved in previous step\n",
    "  * Split the data into training and testing\n",
    "  * Train and test the model\n",
    "  * Upload the results and log metric\n",
    "\n",
    "Another script to create and submit the pipelines ```Pipeline.py```\n",
    "1. Create the environment\n",
    "2. Assign compute clusters\n",
    "3. Create data transfer folder\n",
    "4. Define pipeline steps\n",
    "5. Build the pipeline\n",
    "6. Create/access an experiment\n",
    "7. Run the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace, Experiment, Run, ScriptRunConfig, Environment\n",
    "from azureml.core.environment import CondaDependencies\n",
    "\n",
    "# Access the Workspace\n",
    "ws = Workspace.from_config('./config')\n",
    "\n",
    "# Create and register a custom environment\n",
    "myenv = Environment(name='mytrainingenv')\n",
    "\n",
    "# Provision a Compute Cluster\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "\n",
    "# Configuration of the compute cluster\n",
    "compute_config = AmlCompute.provisioning_configuration(\n",
    "    vm_size='STANDARD_D11_V2',\n",
    "    max_nodes=2\n",
    ")\n",
    "\n",
    "# Create the cluster\n",
    "computer_cluster = ComputeTarget.create(ws, name='my-cluster-001', compute_config=compute_config)\n",
    "\n",
    "computer_cluster.wait_for_completion()\n",
    "\n",
    "# ----------------------------------------------\n",
    "# Create pipelines\n",
    "# ----------------------------------------------\n",
    "\n",
    "# Step 01 - Data Preparation\n",
    "from azureml.core.runconfig import RunConfiguration\n",
    "\n",
    "run_config = RunConfiguration()\n",
    "run_config.target = computer_cluster\n",
    "run_config.environment = myenv\n",
    "\n",
    "# Define the pipeline steps\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "from azureml.pipeline.data import PipelineData\n",
    "\n",
    "input_ds = ws.datasets.get('Defaults')\n",
    "dataFolder = PipelineData('datafolder', datastore=ws.get_default_datastore())\n",
    "\n",
    "data_prep_step = PythonScriptStep(\n",
    "    name='01 Data Preparation',\n",
    "    script_name='DataPrep.py',\n",
    "    compute_target=computer_cluster,\n",
    "    source_directory='.',\n",
    "    inputs=[input_ds.as_named_input('raw_data')],\n",
    "    output=[dataFolder],\n",
    "    runconfig=run_config,\n",
    "    arguments=['--datafolder', dataFolder]\n",
    ")\n",
    "\n",
    "# Step 02 - Build and train model\n",
    "train_step = PythonScriptStep(\n",
    "    name='02 Build and train model',\n",
    "    script_name='Training.py',\n",
    "    compute_target=computer_cluster,\n",
    "    source_directory='.',\n",
    "    inputs=[dataFolder],\n",
    "    runconfig=run_config,\n",
    "    arguments=['--datafolder', dataFolder]\n",
    ")\n",
    "\n",
    "# Configure and build the pipeline\n",
    "steps = [data_prep_step, train_step]\n",
    "\n",
    "from azureml.pipeline.core import Pipeline\n",
    "new_pipeline = Pipeline(workspace=ws, steps=steps)\n",
    "\n",
    "# ----------------------------------------------\n",
    "# Create/access an experiment\n",
    "# ----------------------------------------------\n",
    "from azureml.core import Experiment\n",
    "new_experiment = Experiment(workspace=ws, name='Loan-SDK-Exp01')\n",
    "new_pipeline_run = new_experiment.submit(new_pipeline)\n",
    "\n",
    "new_pipeline_run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Enable command line argument to run the script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparser import ArgumentParser\n",
    "\n",
    "parser = ArgumentParser()\n",
    "\n",
    "# single argument\n",
    "parser.add_argument('--datafolder', type=str, dest='datafolder', help='data folder path')\n",
    "\n",
    "# multiple arguments\n",
    "args = parser.parse_args()\n",
    "\n",
    "# access the argument\n",
    "print('data folder:', args.datafolder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create DataPrep.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Run\n",
    "\n",
    "# Get the run context\n",
    "run = Run.get_context()\n",
    "\n",
    "# Access the workspace\n",
    "ws = run.experiment.workspace\n",
    "\n",
    "# load data to DataFrame\n",
    "df = run.input_datasets['raw_data'].to_pandas_dataframe()\n",
    "\n",
    "# ----------------------------------------------\n",
    "# Data Preparation Code Here\n",
    "# ----------------------------------------------\n",
    "\n",
    "# Get the arguments from pipeline job\n",
    "from argparser import ArgumentParser\n",
    "\n",
    "parser = ArgumentParser()\n",
    "parser.add_argument('--datafolder', type=str)\n",
    "args = parser.parse_args()\n",
    "\n",
    "# create the folder if not exists\n",
    "import os\n",
    "\n",
    "# create the folder if not exists\n",
    "os.makedirs(args.datafolders, exist_ok=True)\n",
    "\n",
    "# save the DataFrame to csv file\n",
    "path = os.path.join(args.datafolders, 'defaults_prep.csv')\n",
    "df.to_csv(path, index=False)\n",
    "\n",
    "# Complete the run\n",
    "run.complete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Run\n",
    "import pandas as pd\n",
    "from argparser import ArgumentParser\n",
    "\n",
    "parser = ArgumentParser()\n",
    "parser.add_argument('--datafolder', type=str)\n",
    "args = parser.parse_args()\n",
    "\n",
    "# Get the run context\n",
    "run = Run.get_context()\n",
    "\n",
    "# Access the workspace\n",
    "ws = run.experiment.workspace\n",
    "\n",
    "# Get the run context\n",
    "run = Run.get_context()\n",
    "\n",
    "# ----------------------------------------------\n",
    "# Training Code Here\n",
    "# ----------------------------------------------\n",
    "# save the DataFrame to csv file\n",
    "path = os.path.join(args.datafolders, 'defaults_prep.csv')\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "# Complete the run\n",
    "run.complete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SDK in Azure Designer\n",
    "\n",
    "Except using SDK to create the pipeline, we also can embeded the Python script in the Azure Designer with Python Script Module. Also can use Zip Bundle to upload the package to the module.\n",
    "\n",
    "Select the Execute the Python Script module, and write the code to execute the script. Save the script select the Run settings to choose a compute. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python Script Module\n",
    "import pandas as pd\n",
    "\n",
    "def azureml_main(dataframe1 = None, dataframe2 = None):\n",
    "    df = dataframe1\n",
    "    df = df.drop(['fw', 'edu_num'], axis=1)\n",
    "\n",
    "    X = df.iloc[:, :-1]\n",
    "    y = df.iloc[:, -1]\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zip Bundle\n",
    "# 1 -- Packed the previous process script into a file as a zip bundle\n",
    "# DataPrep.py\n",
    "from azureml.core import Run\n",
    "\n",
    "# Get the run context\n",
    "run = Run.get_context()\n",
    "\n",
    "# Access the workspace\n",
    "ws = run.experiment.workspace\n",
    "\n",
    "def data_prep():\n",
    "    input_ds = ws.datasets.get('Defaults').to_pandas_dataframe()\n",
    "    input_ds = input_ds.drop(['fw', 'edu_num'], axis=1)\n",
    "    return input_ds\n",
    "\n",
    "# 2 -- Pack the DataPrep.py and other dependencies into a zip file\n",
    "# 3 -- Create a Dataset in the workspace with uploaded packed ZIP file\n",
    "# 4 -- Drage Dataset and create a Python Script Module in the Designer to import the DataPrep.py\n",
    "\n",
    "# Python Script Module\n",
    "import pandas as pd\n",
    "\n",
    "def azureml_main(dataframe1 = None, dataframe2 = None):\n",
    "\n",
    "    import DataPrep\n",
    "    df = DataPrep.data_prep()\n",
    "    return df\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
