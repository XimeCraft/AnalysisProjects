{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import Config\n",
    "\n",
    "from azure.ai.ml import MLClient, command\n",
    "from azure.identity import DefaultAzureCredential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Azure Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "config = Config()\n",
    "\n",
    "# setup azureML authentication\n",
    "ml_client = MLClient(\n",
    "    DefaultAzureCredential(), config.SUBSCIRPTION_ID, config.RESOURCE_GROUP, config.WORKSPACE_NAME\n",
    ")\n",
    "\n",
    "# connecto to azureML workspace\n",
    "job = command(\n",
    "    code=,\n",
    "    command=,\n",
    "    environment=,\n",
    "    compute=\"MLModels\",\n",
    "    experiment_name=\n",
    ")\n",
    "\n",
    "returned_job = ml_client.create_or_update(job)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create datastore\n",
    "\n",
    "**Three types of datastore**:\n",
    "* Azure Blob Storage Container - azureml (protocol)\n",
    "* Azure File share\n",
    "* Azure Data Lake (Gen2) - abf(s) (protocol)\n",
    "\n",
    "### Create data assets\n",
    "\n",
    "**Three types of data assets**: \n",
    "* URI files\n",
    "* URI folder\n",
    "* ML Tables points to file or folders -> include a schema to read as a tabular data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create URI file data asset\n",
    "from azure.ai.ml.entities import Data\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "\n",
    "my_path = '<supported-path>'\n",
    "\n",
    "my_data = Data(\n",
    "    path=my_path,\n",
    "    type=AssetTypes.URI_FILE,\n",
    "    description='<description>',\n",
    "    name='<name>',\n",
    "    version='<version>'\n",
    ")\n",
    "\n",
    "ml_client.data.create_or_update(my_data)\n",
    "\n",
    "# access to input data through URI FILE\n",
    "import argparse\n",
    "import pandas as pd\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--input_data', type=str)\n",
    "args = parser.parse_args()\n",
    "\n",
    "df = pd.read_csv(args.input_data)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create URI FOLDER\n",
    "from azure.ai.ml.entities import Data\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "\n",
    "my_path = '<supported-path'\n",
    "\n",
    "my_data = Data(\n",
    "    path=my_path,\n",
    "    type=AssetTypes.URI_FOLDER,\n",
    "    description='<description>',\n",
    "    name='<name>',\n",
    "    version='<version>'\n",
    ")\n",
    "\n",
    "ml_client.data.create_or_update(my_data)\n",
    "\n",
    "# Access input data through URI FOLDER\n",
    "import argparse\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--input_daeta', type=str)\n",
    "args = parser.parse_args()\n",
    "\n",
    "data_path = args.input_data\n",
    "all_files = glob.glob(data_path + '/*.csv')\n",
    "df = pd.concat((pd.read_csv(f) for f in all_files), srot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creste MLTable\n",
    "from azure.ai.ml.entities import Data\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "\n",
    "my_path = '<path-including-mltable-files>'\n",
    "\n",
    "my_data = Data(\n",
    "    path=my_path,\n",
    "    type=AssetTypes.MLTABLE,\n",
    "    description='<description>',\n",
    "    name='<name>',\n",
    "    version='<version>'\n",
    ")\n",
    "\n",
    "ml_client.data.create_or_update(my_data)\n",
    "\n",
    "# Access data through MLTable\n",
    "from argparse\n",
    "import mltable\n",
    "import pandas as pd\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--input_data', type=str)\n",
    "args = parser.parse_args()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--input_data'], dest='input_data', nargs=None, const=None, default=None, type=<class 'str'>, choices=None, help=None, metavar=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a comput target\n",
    "\n",
    "* Compute Instance - virtual machine -> jupyter notebook -> experiment\n",
    "* Compute Cluster -> large scale dataset -> on demand -> parallel processing\n",
    "* Kebernete Cluster - > use Kebernete technology\n",
    "* Attached Compute -> attached to existing Azure Databricks cluster/Azure VM\n",
    "* Serverless Compute -> on demand, fully managed\n",
    "\n",
    "**When to use>**\n",
    "* experiment : compute instance\n",
    "* production: -> pipeline jobs -> computer cluster / serverless commpute\n",
    "* deployment: \n",
    "  * batch prediction -> compute cluster / serverless compute\n",
    "  * real-time prediciton -> kebernete cluster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create compute instance\n",
    "from azure.ai.ml.entities import ComputeInstance\n",
    "\n",
    "ci_basic_name = \"basic-ci=123\"\n",
    "ci_basic = ComputeInstance(\n",
    "    name=ci_basic_name,\n",
    "    size = \"STANDARD_DS3_v2\"\n",
    ")\n",
    "\n",
    "ml_client.begin_create_or_update(ci_basic).result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a computer cluster\n",
    "from azure.ai.ml.entities import AmlCompute\n",
    "\n",
    "cluster_basic = AmlCompute(\n",
    "    name=\"cpu-cluster\",\n",
    "    type=\"amlcompute\",\n",
    "    size=\"STANDARD_DS3_v2\",\n",
    "    location=\"westus\",\n",
    "    min_instances=0,\n",
    "    max_instances=2,\n",
    "    idle_time_before_scale_down=120,\n",
    "    tier=\"low_priority\"\n",
    ")\n",
    "ml_client.begin_create_or_update(cluster_basic).resuot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **node**: cluster scale to, each node can execute a parallel workloads\n",
    "* ```size```: speicifies virtual machine type of each node of compute cluster\n",
    "* ```max_instance```: number of maximum number of nodes(parallel workloads)\n",
    "* ```tier```: whether low priority or not.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a compute cluster - bind to a job\n",
    "from azure.ai.ml import command\n",
    "\n",
    "# configure a job\n",
    "job = command(\n",
    "    code='./src',\n",
    "    command=\"python diabetes-training.py\",\n",
    "    environment=\"AzureML-sklearn-0.24-ubuntu18.04-py37-cpu@latest\",\n",
    "    compute=\"cpu-cluster\",\n",
    "    display_name=\"train-with-cluster\",\n",
    "    experiment_name=\"diabetes-training\"\n",
    ")\n",
    "\n",
    "# submit job\n",
    "returned_job = ml_client.create_or_update(job)\n",
    "anl_url = returned_job.studio_url\n",
    "aml_url\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment  - Azure Machine Learning Environment \n",
    "**Docker container** -> Environment.\n",
    "\n",
    "\n",
    "1. Curated environment\n",
    "\n",
    "The existing environment will combined to the workspace while creating workspace.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check environemtn\n",
    "envs = ml_client.environments.list()\n",
    "for env in envs:\n",
    "    print(env)\n",
    "\n",
    "# Review details of environment\n",
    "env = ml_client.environments.get(name=\"env-name\", version=\"1\")\n",
    "print(env)\n",
    "\n",
    "# Use a curated environment in job\n",
    "from azure.ai.ml import command\n",
    "\n",
    "job = command(\n",
    "    code=\"./src\",\n",
    "    command=\"python train.py\",\n",
    "    environment=\"AzureML-sklearn-0.24-ubuntu18.04-py37-cpu@latest\",\n",
    "    compute='aml-cluster',\n",
    "    display_name=\"train-with-curated-environment\",\n",
    "    environment_name=\"train-with-curated-environment\"\n",
    ")\n",
    "\n",
    "returned_job = ml_client.create_or_update(job)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Custom environment \n",
    "\n",
    "Can careate a custom environments in Docker Hub through SDK or conda yml file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an environment\n",
    "from azure.ai.ml.entities import Environment\n",
    "\n",
    "env_docker_image = Environment(\n",
    "    image=\"pytorch/pytorch:latest\",\n",
    "    name=\"public-docker-image-example\",\n",
    "    description=\"Environment created from a public Docker image.\"\n",
    ")\n",
    "\n",
    "ml_client.environments.create_or_update(env_docker_image)\n",
    "\n",
    "# Or create an environmemt through base docker image\n",
    "from azure.ai.ml.entities import Environment\n",
    "\n",
    "env_docker_image = Environment(\n",
    "    image=\"mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04\",\n",
    "    name=\"aml-docker-image-example\",\n",
    "    description=\"Environment created from a Azure ML Docker image.\",\n",
    ")\n",
    "ml_client.environments.create_or_update(env_docker_image)\n",
    "\n",
    "# Or create an environment with conda specification file\n",
    "from azure.ai.ml.entities import Environment\n",
    "\n",
    "env_docker_conda = Environment(\n",
    "    image=\"mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04\",\n",
    "    conda_file=\"./conda-env.yml\",\n",
    "    name=\"docker-image-plus-conda-example\",\n",
    "    description=\"Environment created from a Docker image plus Conda environment.\",\n",
    ")\n",
    "ml_client.environments.create_or_update(env_docker_conda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "xml"
    }
   },
   "outputs": [],
   "source": [
    "# example of yml used in conda file\n",
    "name: basic-env-cpu\n",
    "channels:\n",
    "  - conda-forge\n",
    "dependencies:\n",
    "  - python=3.7\n",
    "  - scikit-learn\n",
    "  - pandas\n",
    "  - numpy\n",
    "  - matplotlib"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
